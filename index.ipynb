{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll learn about different common activation functions, and compare and contrast their effectiveness on an MLP for classification on the MNIST data set!\n",
    "\n",
    "### Getting Started: What Is An Activation Function?\n",
    "\n",
    "In your words, answer the following question:\n",
    "\n",
    "**_What purpose do acvtivation functions serve in Deep Learning?  What happens if our neural network has no activation functions?  What role do activation functions play in our output layer? Which activation functions are most commonly used in an output layer?_**\n",
    "\n",
    "Write your answer below this line:\n",
    "______________________________________________________________________________________________________________________\n",
    "**Activation functions allow our Deep Learning models to capture nonlinearity. If ANNs are a symbolic representation of biological neural networks, then activation functions mirror the ability of neurons being able to fire with different levels of intensity based on the rapidity of how often they fire. A model with no activation functions would just be a linear model. In the output layer, activation functions make the results of our neural network's forward propagation step interpretable. If the task we are trying to solve is a binary classification task, then we would use a sigmoid neuron, so that we can interpret the results as a probability, much like the output of a logistic regression. If our task is multi-class classification, then we would use a softmax function, which would have the network output a vector of probabilities, which each element corresponding to the probability that the observed input data belongs to a different class.**\n",
    "\n",
    "For the first part of this lab, we'll only make use of the numpy library.  Run the cell below to import numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Different Activation Functions\n",
    "\n",
    "We'll begin this lab by writing different activation functions manually, so that we can get a feel for how they work.  \n",
    "\n",
    "### Logistic Sigmoid Function\n",
    "\n",
    "\n",
    "We'll begin with the **_Sigmoid_** activation function, as described by the following equation:\n",
    "\n",
    "$$\\LARGE \\phi(z) = \\frac{1}{1 + e^{-z}}  $$\n",
    "\n",
    "In the cell below, complete the `sigmoid` function. This functio should take in a value and compute the results of the equation returned above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6125396134409151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(.458) # Expected Output 0.61253961344091512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent (tanh) Function \n",
    "\n",
    "The hyperbolic tangent function is as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$\\LARGE  \\frac{e^x - e^{-x}}{e^x + e^{-x}}  $$\n",
    "\n",
    "Complete the function below by implementing the `tanh` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964027580075817\n",
      "0.9640275800758169\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tanh(2)) # 0.964027580076\n",
    "print(np.tanh(2)) # 0.964027580076\n",
    "print(tanh(0)) # 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU) Function\n",
    "\n",
    "The final activation function we'll implement manually is the **_Rectified Linear Unit_** function, also known as **_ReLU_**.  \n",
    "\n",
    "The relu function is:\n",
    "\n",
    "$$\\LARGE  Max(0, z)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return max(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(relu(-2)) # Expected Result: 0.0\n",
    "print(relu(2)) # Expected Result: 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "The **_Softmax Function_** is primarily used as the activation function on the output layer for neural networks for multi-class categorical prediction.  The softmax equation is as follows:\n",
    "\n",
    "<img src='softmax.png'>\n",
    "\n",
    "The mathematical notation for the softmax activation function is a bit dense, and this is a special case, since the softmax function is really only used on the output layer. Thus, the code for the softmax function ahs been provided.  \n",
    "\n",
    "Run the cell below to compute the softmax function on a sample vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02364054, 0.06426166, 0.1746813 , 0.474833  , 0.02364054,\n",
       "       0.06426166, 0.1746813 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "softmax = np.exp(z)/np.sum(np.exp(z))\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Expected Output:_**\n",
    "\n",
    "array([ 0.02364054,  0.06426166,  0.1746813 ,  0.474833  ,  0.02364054,\n",
    "        0.06426166,  0.1746813 ])\n",
    "\n",
    "\n",
    "## Comparing Training Results \n",
    "\n",
    "Now that we have experience with the various activation functions, we'll gain some practical experience with each of them by trying them all as different hyperparameters in a neural network to see how they affect the performance of the model. Before we can do that, we'll need to preprocess our image data. \n",
    "\n",
    "We'll build 3 different versions of the same network, with the only difference between them being the activation function used in our hidden layers.  Start off by importing everything we'll need from Keras in the cell below.\n",
    "\n",
    "**_HINT:_** Refer to previous labs that make use of Keras if you aren't sure what you need to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Our Image Data\n",
    "\n",
    "We'll need to preprocess the MNIST image data so that it can be used in our model. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Load the training and testing data and their corresponding labels from MNIST.  \n",
    "* Reshape the data inside `X_train` and `X_test` into the appropriate shape (from a 28x28 matrix to a vector of length 784).  Also cast them to datatype `float32`.\n",
    "* Normalize the data inside of `X_train` and `X_test`\n",
    "* Convert the labels inside of `y_train` and `y_test` into one-hot vectors (Hint: see the [documentation](https://keras.io/utils/#to_categorical) if you can't remember how to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000,784).astype('float32')\n",
    "X_test = X_test.reshape(10000,784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train,10)\n",
    "y_test = keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Your task is to build a neural network to classify the MNIST dataset.  The model should have the following architecture:\n",
    "\n",
    "* Input layer of `(784,)`\n",
    "* Hidden Layer 1: 100 neurons\n",
    "* Hidden Layer 2: 50 neurons\n",
    "* Output Layer: 10 neurons, softmax activation function\n",
    "* Loss: `categorical_crossentropy`\n",
    "* Optimizer: `'SGD'`\n",
    "* metrics:  `['accuracy']`\n",
    "\n",
    "In the cell below, create a model that matches the specifications above and use a **_sigmoid activation function for all hidden layers_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_model = Sequential()\n",
    "\n",
    "sigmoid_model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "sigmoid_model.add(Dense(50, activation='sigmoid'))\n",
    "sigmoid_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sigmoid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile the model with the following hyperparameters:\n",
    "\n",
    "* `loss='categorical_crossentropy'`\n",
    "* `optimizer='SGD'`\n",
    "* `metrics=['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the model.  In addition to our training data, pass in the following parameters:\n",
    "\n",
    "* `epochs=10`\n",
    "* `batch_size=32`\n",
    "* `verbose=1`\n",
    "* `validation_data=(X_test, y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 2.2097 - acc: 0.3622 - val_loss: 2.0660 - val_acc: 0.5111\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.7687 - acc: 0.6208 - val_loss: 1.3955 - val_acc: 0.6871\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 1.1274 - acc: 0.7453 - val_loss: 0.8987 - val_acc: 0.7942\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.7944 - acc: 0.8127 - val_loss: 0.6843 - val_acc: 0.8305\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6370 - acc: 0.8419 - val_loss: 0.5705 - val_acc: 0.8554\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.5470 - acc: 0.8589 - val_loss: 0.5004 - val_acc: 0.8695\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4893 - acc: 0.8705 - val_loss: 0.4525 - val_acc: 0.8785\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4488 - acc: 0.8794 - val_loss: 0.4185 - val_acc: 0.8853\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4186 - acc: 0.8864 - val_loss: 0.3934 - val_acc: 0.8886\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.3952 - acc: 0.8913 - val_loss: 0.3730 - val_acc: 0.8958\n"
     ]
    }
   ],
   "source": [
    "sigmoid_model = sigmoid_model.fit(X_train, y_train, batch_size=32, epochs=10, \n",
    "                                  verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Model with Tanh Activations\n",
    "\n",
    "Now, we'll build the exact same model as we did above, but with hidden layers that use `tanh` activation functions rather than `sigmoid`.\n",
    "\n",
    "In the cell below, create a second version of the model that uses hyperbolic tangent function for activations.  All other parameters, including number of hidden layers, size of hidden layers, and the output layer should remain the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_model = Sequential()\n",
    "\n",
    "tanh_model.add(Dense(100, activation='tanh', input_shape=(784,)))\n",
    "tanh_model.add(Dense(50, activation='tanh'))\n",
    "tanh_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tanh_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile this model.  Use the same hyperparameters as we did for the sigmoid model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the model.  Use the same hyperparameters as we did for the sigmoid model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.6175 - acc: 0.8421 - val_loss: 0.3559 - val_acc: 0.9048\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3279 - acc: 0.9088 - val_loss: 0.2874 - val_acc: 0.9212\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2768 - acc: 0.9213 - val_loss: 0.2534 - val_acc: 0.9304\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2453 - acc: 0.9298 - val_loss: 0.2281 - val_acc: 0.9367\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2217 - acc: 0.9367 - val_loss: 0.2105 - val_acc: 0.9408\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.2025 - acc: 0.9427 - val_loss: 0.1954 - val_acc: 0.9457\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.1866 - acc: 0.9465 - val_loss: 0.1818 - val_acc: 0.9481\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1732 - acc: 0.9506 - val_loss: 0.1714 - val_acc: 0.9517\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.1613 - acc: 0.9539 - val_loss: 0.1616 - val_acc: 0.9531\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.1512 - acc: 0.9564 - val_loss: 0.1534 - val_acc: 0.9552\n"
     ]
    }
   ],
   "source": [
    "tanh_model = tanh_model.fit(X_train, y_train, batch_size=32, epochs=10, \n",
    "                            verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Model with ReLU Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, construct a third version of the same model, but this time with `relu` activation functions for the hidden layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model = Sequential()\n",
    "\n",
    "relu_model.add(Dense(100, activation='relu', input_shape=(784,)))\n",
    "relu_model.add(Dense(50, activation='relu'))\n",
    "relu_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 84,060\n",
      "Trainable params: 84,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "relu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile the model with the same hyperparameters as the last two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the model with the same hyperparameters as the last two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.6656 - acc: 0.8195 - val_loss: 0.3345 - val_acc: 0.9066\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.3039 - acc: 0.9128 - val_loss: 0.2595 - val_acc: 0.9248\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.2491 - acc: 0.9281 - val_loss: 0.2201 - val_acc: 0.9353\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.2152 - acc: 0.9388 - val_loss: 0.1924 - val_acc: 0.9436\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.1897 - acc: 0.9456 - val_loss: 0.1754 - val_acc: 0.9489\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.1705 - acc: 0.9515 - val_loss: 0.1620 - val_acc: 0.9520\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.1548 - acc: 0.9549 - val_loss: 0.1536 - val_acc: 0.9562\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.1414 - acc: 0.9598 - val_loss: 0.1371 - val_acc: 0.9595\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.1305 - acc: 0.9629 - val_loss: 0.1312 - val_acc: 0.9610\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.1204 - acc: 0.9655 - val_loss: 0.1265 - val_acc: 0.9620\n"
     ]
    }
   ],
   "source": [
    "relu_model = relu_model.fit(X_train, y_train, batch_size=32, epochs=10,\n",
    "                            verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Which activation function was most effective?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relu model was most effective.\n",
    "- Sigmoid model was the least effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
